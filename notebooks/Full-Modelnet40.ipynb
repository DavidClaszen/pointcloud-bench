{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fec43956",
   "metadata": {},
   "source": [
    "# Full Modelnet40\n",
    "\n",
    "This script preprocesses the aligned ModelNet40 dataset from [ORION](https://github.com/lmb-freiburg/orion) and produces a new dataset that is compatible for training PAPNet with non-partial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef1519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6039554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_3d(\n",
    "    sample: np.array,\n",
    "    show_axes: bool = True\n",
    "):\n",
    "    \"\"\"Visualizes point clouds with normals in 3D plot.\n",
    "\n",
    "    Args:\n",
    "        sample (np.array): (points, 6) where points=point clouds,\n",
    "            6 = 3 dimensions + 3 normals\n",
    "        i (int): n index of sample to visualize\n",
    "    \"\"\"\n",
    "    points = sample[:, :3]\n",
    "    normals = sample[:, 3:]\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd.normals = o3d.utility.Vector3dVector(normals)\n",
    "\n",
    "    if show_axes:\n",
    "        Markers = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.05, origin=[0, 0, 0])\n",
    "        o3d.visualization.draw_geometries([pcd, Markers], width=800, height=600)\n",
    "    else:\n",
    "        o3d.visualization.draw_geometries([pcd], width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5358502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_to_point_cloud(file_path:str, num_points:int=1024) -> np.ndarray:\n",
    "    \"\"\"Load a 3D mesh from an .off file and sample points uniformly from its surface.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the .off file.\n",
    "        num_points (int): Number of points to sample from the mesh surface.\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (num_points, 6) containing the sampled point cloud, with XYZ coordinates and normals.\n",
    "    \"\"\"\n",
    "    mesh = o3d.io.read_triangle_mesh(file_path)\n",
    "    mesh.compute_vertex_normals()\n",
    "    pcd = mesh.sample_points_uniformly(number_of_points=num_points)\n",
    "    points = np.asarray(pcd.points)\n",
    "    normals = np.asarray(pcd.normals)\n",
    "    return np.hstack((points, normals))\n",
    "\n",
    "def pc_normalize(pc:np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Normalize a point cloud to be centered at the origin and fit within a unit sphere.\n",
    "\n",
    "    Args:\n",
    "        pc (np.ndarray): Array of shape (num_points, 6) containing the point cloud.\n",
    "    Returns:\n",
    "        np.ndarray: Normalized point cloud of shape (num_points, 6).\n",
    "    \"\"\"\n",
    "    centroid = np.mean(pc[:, :3], axis=0)\n",
    "    pc[:, :3] -= centroid\n",
    "    max_dist = np.max(np.sqrt(np.sum(pc[:, :3] ** 2, axis=1)))\n",
    "    pc[:, :3] /= max_dist\n",
    "    return pc\n",
    "\n",
    "def batch_create_pointclouds_from_dir(base_path:str, num_points:int=1024) -> np.ndarray:\n",
    "    \"\"\"Create a numpy array of point clouds from .off files in the specified directory.\n",
    "\n",
    "        Args:\n",
    "        base_path (str): Directory containing .off files.\n",
    "        num_points (int): Number of points to sample from each mesh surface.\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (N, num_points, 6) containing N point clouds.\n",
    "        list[str]: List of original filenames corresponding to each point cloud.\n",
    "    \"\"\"\n",
    "    off_files = sorted([f for f in os.listdir(base_path) if f.endswith('.off')])\n",
    "    point_clouds = np.zeros((len(off_files), num_points, 6), dtype=np.float32)\n",
    "\n",
    "    pbar = tqdm(total=len(off_files), desc=f\"{base_path}..\")\n",
    "\n",
    "    for i, off_file in enumerate(off_files):\n",
    "        pbar.update(1)\n",
    "        file_path = os.path.join(base_path, off_file)\n",
    "        try:\n",
    "            pc = off_to_point_cloud(file_path, num_points)\n",
    "            pc = pc_normalize(pc)\n",
    "            point_clouds[i] = pc\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            continue\n",
    "    pbar.close()\n",
    "        \n",
    "    return point_clouds, [f[:-4] for f in off_files]  # return filenames without .off extension\n",
    "\n",
    "def generate_random_translation_vector_normal(length, translation_u_stddev: list[tuple[float, float]] = [(0, 0.1), (0, 0.1), (0, 0.1)]) -> np.ndarray:\n",
    "    \"\"\"Generate a random translation vector sampled from normal distributions for each axis.\n",
    "\n",
    "    Args:\n",
    "        translation_u_stddev (list[tuple[float, float]]): List of 3 tuples specifying the mean and stddev for each axis (x, y, z).\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (length, 3) containing translation vectors.\n",
    "    \"\"\"\n",
    "    if length < 1:\n",
    "        raise ValueError(\"length must be >= 1\")\n",
    "    \n",
    "    translations = np.empty((length, 3), dtype=np.float64)\n",
    "    for i in range(3):\n",
    "        mean, stddev = translation_u_stddev[i]\n",
    "        translations[:, i] = np.random.normal(mean, stddev, length)\n",
    "    \n",
    "    return translations\n",
    "\n",
    "def generate_random_translation_vector(length, translation_range: list[tuple[float, float]] = [(-1, 1), (-1, 1), (-1, 1)]) -> np.ndarray:\n",
    "    \"\"\"Generate a random translation vector within specified ranges for each axis.\n",
    "\n",
    "    Args:\n",
    "        translation_range (list[tuple[float, float]]): List of 3 tuples specifying the min and max translation for each axis (x, y, z).\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (length, 3) containing translation vectors.\n",
    "    \"\"\"\n",
    "    if length < 1:\n",
    "        raise ValueError(\"length must be >= 1\")\n",
    "    \n",
    "    translations = np.empty((length, 3), dtype=np.float64)\n",
    "    for i in range(3):\n",
    "        min_val, max_val = translation_range[i]\n",
    "        translations[:, i] = np.random.uniform(min_val, max_val, length)\n",
    "    \n",
    "    return translations\n",
    "\n",
    "def generate_random_rotation_matrix(length: int = 1) -> np.ndarray:\n",
    "    \"\"\"Generate random rotation matrices uniformly sampled from SO(3).\n",
    "\n",
    "    Shoemake (1992) method is used to sample uniform quaternions, which are then converted to rotation matrices.\n",
    "\n",
    "    Args:\n",
    "        length (int): Number of rotation matrices to generate.\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (length, 3, 3) containing rotation matrices.\n",
    "    \"\"\"\n",
    "    if length < 1:\n",
    "        raise ValueError(\"length must be >= 1\")\n",
    "\n",
    "    # Shoemake (1992) method: sample uniform quaternions\n",
    "    u1 = np.random.rand(length)\n",
    "    u2 = np.random.rand(length)\n",
    "    u3 = np.random.rand(length)\n",
    "\n",
    "    q1 = np.sqrt(1.0 - u1) * np.sin(2.0 * np.pi * u2)\n",
    "    q2 = np.sqrt(1.0 - u1) * np.cos(2.0 * np.pi * u2)\n",
    "    q3 = np.sqrt(u1)        * np.sin(2.0 * np.pi * u3)\n",
    "    q4 = np.sqrt(u1)        * np.cos(2.0 * np.pi * u3)\n",
    "\n",
    "    # Interpret as quaternion (w, x, y, z)\n",
    "    x = q1\n",
    "    y = q2\n",
    "    z = q3\n",
    "    w = q4\n",
    "\n",
    "    xx = x * x\n",
    "    yy = y * y\n",
    "    zz = z * z\n",
    "    xy = x * y\n",
    "    xz = x * z\n",
    "    yz = y * z\n",
    "    wx = w * x\n",
    "    wy = w * y\n",
    "    wz = w * z\n",
    "\n",
    "    rotations = np.empty((length, 3, 3), dtype=np.float64)\n",
    "\n",
    "    rotations[:, 0, 0] = 1.0 - 2.0 * (yy + zz)\n",
    "    rotations[:, 0, 1] = 2.0 * (xy - wz)\n",
    "    rotations[:, 0, 2] = 2.0 * (xz + wy)\n",
    "\n",
    "    rotations[:, 1, 0] = 2.0 * (xy + wz)\n",
    "    rotations[:, 1, 1] = 1.0 - 2.0 * (xx + zz)\n",
    "    rotations[:, 1, 2] = 2.0 * (yz - wx)\n",
    "\n",
    "    rotations[:, 2, 0] = 2.0 * (xz - wy)\n",
    "    rotations[:, 2, 1] = 2.0 * (yz + wx)\n",
    "    rotations[:, 2, 2] = 1.0 - 2.0 * (xx + yy)\n",
    "\n",
    "    return rotations\n",
    "\n",
    "def create_random_rot_and_trans(points:np.ndarray,\n",
    "                                translation_param: list[tuple[float, float]]) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Apply random rotation matrices and translation vectors for a batch of point clouds.\n",
    "    The translation vectors are sampled from normal distributions specified by translation_param (mean, stddev) for each axis.\n",
    "    The rotation matrices are sampled uniformly from SO(3).\n",
    "\n",
    "    Args:\n",
    "        points (np.ndarray): Array of shape (N, num_points, 6) containing N point clouds.\n",
    "        translation_param (list[tuple[float, float]]): List of 3 tuples specifying the mean and stddev for each axis (x, y, z).\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray, np.ndarray]: \n",
    "            - Array of shape (N, num_points, 6) containing the transformed point clouds.\n",
    "            - Array of shape (N, 3) containing translation vectors.\n",
    "            - Array of shape (N, 3, 3) containing rotation matrices.\n",
    "    \"\"\"\n",
    "    batch_size = points.shape[0]\n",
    "    \n",
    "    # Initialize arrays to store transformations\n",
    "    translations = generate_random_translation_vector_normal(batch_size, translation_param)    \n",
    "    rotations = generate_random_rotation_matrix(batch_size)\n",
    "    \n",
    "    # Apply rotation and translation using batch matrix multiplication\n",
    "    points[:, :, :3] = points[:, :, :3] @ rotations.transpose(0, 2, 1) + translations[:, np.newaxis, :]\n",
    "\n",
    "    return points, translations, rotations\n",
    "\n",
    "def get_pc_stats(pc: np.array):\n",
    "    \"\"\"Get point cloud statistics per dimension.\n",
    "\n",
    "    Args:\n",
    "        pc (np.array): (points, 3) point cloud data\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with mean, min, max, stddev of point cloud\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    stats['mean'] = pc.mean(axis=0)\n",
    "    stats['min'] = pc.min(axis=0)\n",
    "    stats['max'] = pc.max(axis=0)\n",
    "    stats['stddev'] = pc.std(axis=0)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0be1864",
   "metadata": {},
   "source": [
    "### Tests for the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625532cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_off_to_point_cloud():\n",
    "    base_path = \"../datasets/modelnet40_auto_aligned/airplane/train/airplane_0001.off\"\n",
    "    pc = off_to_point_cloud(base_path, num_points=1024)\n",
    "    assert pc.shape == (1024, 6), f\"Expected shape (1024, 6), got {pc.shape}\"\n",
    "    print(\"off_to_point_cloud test passed.\")\n",
    "    visualize_3d(pc, show_axes=True)\n",
    "\n",
    "def test_batch_create_pointclouds_from_dir():\n",
    "    base_path = \"../datasets/modelnet40_auto_aligned/airplane/test/\"\n",
    "    pcs, filenames = batch_create_pointclouds_from_dir(base_path, num_points=1024)\n",
    "    assert pcs.shape[1:] == (1024, 6), f\"Expected shape (_, 1024, 6), got {pcs.shape}\"\n",
    "    assert len(filenames) == pcs.shape[0], f\"Expected {pcs.shape[0]} filenames, got {len(filenames)}\"\n",
    "    print(\"batch_create_pointclouds_from_dir test passed.\")\n",
    "    visualize_3d(pcs[0], show_axes=True)\n",
    "\n",
    "def test_generate_random_translation_vector():\n",
    "    translations = generate_random_translation_vector(5, [(-1, 1), (-2, 2), (-3, 3)])\n",
    "    assert translations.shape == (5, 3), f\"Expected shape (5, 3), got {translations.shape}\"\n",
    "    for i in range(3):\n",
    "        min_val, max_val = [(-1, 1), (-2, 2), (-3, 3)][i]\n",
    "        assert np.all(translations[:, i] >= min_val) and np.all(translations[:, i] <= max_val), f\"Translations out of range for axis {i}\"\n",
    "    print(\"generate_random_translation_vector test passed.\")\n",
    "\n",
    "def test_generate_random_rotation_matrix():\n",
    "    rotations = generate_random_rotation_matrix(5)\n",
    "    assert rotations.shape == (5, 3, 3), f\"Expected shape (5, 3, 3), got {rotations.shape}\"\n",
    "    for i in range(5):\n",
    "        rt = rotations[i].T @ rotations[i]\n",
    "        identity = np.eye(3)\n",
    "        assert np.allclose(rt, identity), f\"Rotation matrix {i} is not orthogonal.\"\n",
    "        det = np.linalg.det(rotations[i])\n",
    "        assert np.isclose(det, 1.0), f\"Rotation matrix {i} does not have determinant 1.\"\n",
    "    print(\"generate_random_rotation_matrix test passed.\")\n",
    "\n",
    "def test_generate_random_rot_and_trans():\n",
    "    n_objs = 5\n",
    "    n_points = 1024\n",
    "    pc = np.random.rand(n_objs, n_points, 6)\n",
    "    translation_param = [(0, 0.1), (0, 0.1), (0, 0.1)]\n",
    "    pc_transformed, translations, rotations = create_random_rot_and_trans(pc, translation_param)\n",
    "    assert pc_transformed.shape == (n_objs, n_points, 6), f\"Expected shape ({n_objs}, {n_points}, 6), got {pc_transformed.shape}\"\n",
    "    assert translations.shape == (n_objs, 3), f\"Expected shape ({n_objs}, 3), got {translations.shape}\"\n",
    "    assert rotations.shape == (n_objs, 3, 3), f\"Expected shape ({n_objs}, 3, 3), got {rotations.shape}\"\n",
    "    # Verify that each rotation matrix is valid\n",
    "    for i in range(n_objs):\n",
    "        rt = rotations[i].T @ rotations[i]\n",
    "        identity = np.eye(3)\n",
    "        assert np.allclose(rt, identity), f\"Rotation matrix {i} is not orthogonal.\"\n",
    "        det = np.linalg.det(rotations[i])\n",
    "        assert np.isclose(det, 1.0), f\"Rotation matrix {i} does not have determinant 1.\"\n",
    "    print(\"generate_random_rot_and_trans test passed.\")\n",
    "\n",
    "# Uncomment to run tests\n",
    "# test_off_to_point_cloud()\n",
    "# test_batch_create_pointclouds_from_dir()\n",
    "# test_generate_random_translation_vector()\n",
    "# test_generate_random_rotation_matrix()\n",
    "# test_generate_random_rot_and_trans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c791f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline to make a dataset from the manually aligned ModelNet40, sampled using sample_points_uniformly, and normalized using max sum of squares (in pc_normalize)\n",
    "\n",
    "def create_pc_dataset_from_modelnet40(base_path:str, split:str=\"train\",\n",
    "                              num_points:int=1024, num_augmentations=1,\n",
    "                              translation_param: list[tuple[float, float]] = [(0, 0.1), (0, 0.1), (0, 0.1)]) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Create a normalized dataset from the manually aligned ModelNet40 dataset.\n",
    "\n",
    "    The pipeline is as follows:\n",
    "    1. Load .off files from the specified base directory.\n",
    "    2. Sample points uniformly from each mesh surface.\n",
    "    3. Normalize each point cloud to be centered at the origin and fit within a unit sphere.\n",
    "    4. Apply random rotation and translation to each point cloud.\n",
    "        -> for \"train\" split, create 10 random augmentations per sample\n",
    "        -> for \"test\" split, create 1 augmentation per sample\n",
    "    5. Assign integer labels based on category subdirectory names (sorted alphabetically).\n",
    "\n",
    "    Args:\n",
    "        base_path (str): Base directory containing category subdirectories with .off files, organized as follows\n",
    "            ```\n",
    "            base_path/category1/train/*.off\n",
    "            base_path/category1/test/*.off\n",
    "            base_path/category2/train/*.off\n",
    "            base_path/category2/test/*.off\n",
    "            ...\n",
    "            ```\n",
    "        split (str): Dataset split to use (\"train\" or \"test\").\n",
    "        num_augmentations (int): Number of augmentations to create per sample.\n",
    "        num_points (int): Number of points to sample from each mesh surface.\n",
    "        translation_param (list[tuple[float, float]]): List of 3 tuples specifying the mean and stddev for each axis (x, y, z).\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray, list[str]]:\n",
    "            - Array of shape (N, num_points, 6) containing N normalized point clouds.\n",
    "            - Array of shape (N, 3) containing translation vectors.\n",
    "            - Array of shape (N, 3, 3) containing rotation matrices.\n",
    "            - Array of shape (N,) containing integer labels for each point cloud.\n",
    "            - list of original filenames corresponding to each point cloud.\n",
    "    \"\"\"\n",
    "    categories = sorted([d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))])\n",
    "        \n",
    "    all_point_clouds = []\n",
    "    all_translations = []\n",
    "    all_rotations = []\n",
    "    all_labels = []\n",
    "    all_filenames = []\n",
    "    \n",
    "    for cat_idx, category in enumerate(categories):\n",
    "        cat_split_path = os.path.join(base_path, category, split)\n",
    "        \n",
    "        if not os.path.exists(cat_split_path):\n",
    "            print(f\"Warning: {cat_split_path} does not exist, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Load and normalize point clouds for this category\n",
    "        pcs, filenames = batch_create_pointclouds_from_dir(cat_split_path, num_points)\n",
    "        \n",
    "        if len(pcs) == 0:\n",
    "            continue\n",
    "        \n",
    "        # For each point cloud, create num_augmentations copies\n",
    "        pcs_aug = np.repeat(pcs, num_augmentations, axis=0)\n",
    "\n",
    "        pcs_transformed, translations, rotations = create_random_rot_and_trans(pcs_aug, translation_param)\n",
    "        all_point_clouds.append(pcs_transformed)\n",
    "        all_translations.append(translations)\n",
    "        all_rotations.append(rotations)\n",
    "        all_labels.extend([cat_idx] * len(pcs_transformed))\n",
    "        if num_augmentations > 1:\n",
    "            all_filenames.extend([f\"{fname}_{i}\" for fname in filenames for i in range(num_augmentations)])\n",
    "        else:\n",
    "            all_filenames.extend(filenames)\n",
    "\n",
    "    \n",
    "    # Concatenate all results\n",
    "    all_point_clouds = np.concatenate(all_point_clouds, axis=0)\n",
    "    all_translations = np.concatenate(all_translations, axis=0)\n",
    "    all_rotations = np.concatenate(all_rotations, axis=0)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    return all_point_clouds, all_translations, all_rotations, all_labels, all_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a20a21",
   "metadata": {},
   "source": [
    "### Create the fullmodelnet40 dataset from `modelnet40_auto_aligned`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0f9b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the dataset creation pipeline\n",
    "base_path = \"../datasets/modelnet40_auto_aligned/\"\n",
    "out_path = \"../datasets/fullmodelnet40/\"\n",
    "\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "# The stats are from the partialnet40 dataset in the PAPNet paper\n",
    "# used so that the datasets are comparable\n",
    "mean_trans = [0.00299775, 0.00311312, 0.0374197]\n",
    "std_trans = [0.02494127, 0.02498563, 0.03475482]\n",
    "translation_param = [(mean_trans[i], std_trans[i]) for i in range(3)]\n",
    "\n",
    "# Train\n",
    "point_clouds, translations, rotations, labels, filenames = create_pc_dataset_from_modelnet40(base_path, split=\"train\", num_augmentations=10, num_points=1024, translation_param=translation_param)\n",
    "print(f\"Created train dataset with {point_clouds.shape[0]} samples.\")\n",
    "print(\"point_clouds.shape:\", point_clouds.shape)\n",
    "print(\"translations.shape:\", translations.shape)\n",
    "print(\"rotations.shape:\", rotations.shape)\n",
    "print(\"labels.shape:\", labels.shape)\n",
    "# write to .npy files\n",
    "np.save(os.path.join(out_path, \"train_points.npy\"), point_clouds)\n",
    "np.save(os.path.join(out_path, \"train_gt_tra.npy\"), translations)\n",
    "np.save(os.path.join(out_path, \"train_gt_rot.npy\"), rotations)\n",
    "np.save(os.path.join(out_path, \"train_labels.npy\"), labels)\n",
    "# save filenames as text file\n",
    "with open(os.path.join(out_path, \"train_filenames.txt\"), \"w\") as f:\n",
    "    for filename in filenames:\n",
    "        f.write(f\"{filename}\\n\")\n",
    "\n",
    "# Test\n",
    "point_clouds, translations, rotations, labels, filenames = create_pc_dataset_from_modelnet40(base_path, split=\"test\", num_augmentations=1, num_points=1024, translation_param=translation_param)\n",
    "print(f\"Created test dataset with {point_clouds.shape[0]} samples.\")\n",
    "print(\"point_clouds.shape:\", point_clouds.shape)\n",
    "print(\"translations.shape:\", translations.shape)\n",
    "print(\"rotations.shape:\", rotations.shape)\n",
    "print(\"labels.shape:\", labels.shape)\n",
    "# write to .npy files\n",
    "np.save(os.path.join(out_path, \"test_points.npy\"), point_clouds)\n",
    "np.save(os.path.join(out_path, \"test_gt_tra.npy\"), translations)\n",
    "np.save(os.path.join(out_path, \"test_gt_rot.npy\"), rotations)\n",
    "np.save(os.path.join(out_path, \"test_labels.npy\"), labels)\n",
    "# save filenames as text file\n",
    "with open(os.path.join(out_path, \"test_filenames.txt\"), \"w\") as f:\n",
    "    for filename in filenames:\n",
    "        f.write(f\"{filename}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250229a7",
   "metadata": {},
   "source": [
    "### \"Unrotate\" and \"untranslate\" a sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c170ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = 952\n",
    "obj1 = point_clouds[i, :, :3]\n",
    "tra = translations[i]\n",
    "rot = rotations[i]\n",
    "obj1_canonical = (obj1 - tra) @ rot\n",
    "obj1_final = np.concatenate([obj1_canonical, point_clouds[i, :, 3:]], axis=1)\n",
    "\n",
    "stats = get_pc_stats(point_clouds[0][:, :3])\n",
    "print(stats)\n",
    "\n",
    "visualize_3d(point_clouds[i], show_axes=True)\n",
    "visualize_3d(obj1_final, show_axes=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcb-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
